<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>the Origin of Cohesive Subgraph</title>
    <url>/collection/Cohesive-Subgraph/</url>
    <content><![CDATA[<p>A collection note about the origin of the study of cohesive subgraph search</p>
<span id="more"></span>

<center><b><font size="9>Cohesive" Subgraph="" Search</font=""></font></b></center>

<h1 id="1-Preface"><a href="#1-Preface" class="headerlink" title="1 Preface"></a>1 Preface</h1><ul>
<li>All of the data is from Internet or webpages, so I can’t make sure that all of the information is right. If there are some errors, please leave your advice and evidence in issue, I will revise the document as immediately as possible, which is based on your right or convincing advice and evidence.</li>
<li>It is not easy and troublesome to organize the data. Please give me a <code>star</code> if you like it.</li>
</ul>
<h1 id="2-Paper"><a href="#2-Paper" class="headerlink" title="2  Paper"></a>2  Paper</h1><h2 id="2-1-Degree-based-relaxed-clique-models"><a href="#2-1-Degree-based-relaxed-clique-models" class="headerlink" title="2.1 Degree-based relaxed clique models"></a>2.1 Degree-based relaxed clique models</h2><h3 id="2-1-1-K-core"><a href="#2-1-1-K-core" class="headerlink" title="2.1.1  K-core"></a>2.1.1  K-core</h3><p>Paper1: <a href="https://ieeexplore.ieee.org/document/5767911">Efficient core decomposition in massive networks</a><br>Conference/Journal: ICDE, 2011<br>Paper2: <a href="https://ieeexplore.ieee.org/document/6613492">Efficient Core Maintenance in Large Dynamic Graphs</a><br>Conference/Journal: TKDE, 2013</p>
<h3 id="2-1-2-K-plex"><a href="#2-1-2-K-plex" class="headerlink" title="2.1.2  K-plex"></a>2.1.2  K-plex</h3><p>Paper: <a href="https://www.tandfonline.com/doi/abs/10.1080/0022250X.1978.9989883">A graph‐theoretic generalization of the clique concept</a><br>Conference/Journal: The Journal of Mathematical Sociology, 1978</p>
<h2 id="2-2-Distance-based-relaxed-clique-models"><a href="#2-2-Distance-based-relaxed-clique-models" class="headerlink" title="2.2 Distance-based relaxed clique models"></a>2.2 Distance-based relaxed clique models</h2><h3 id="2-2-1-R-clique"><a href="#2-2-1-R-clique" class="headerlink" title="2.2.1 R-clique"></a>2.2.1 R-clique</h3><p>Paper: <a href="https://doi.org/10.14778/2021017.2021025">Keyword search in graphs: finding r-cliques</a><br>Conference/Journal: Proceedings of the VLDB Endowment, 2011</p>
<h3 id="2-2-2-R-club"><a href="#2-2-2-R-club" class="headerlink" title="2.2.2 R-club"></a>2.2.2 R-club</h3><p>Paper: <a href="http://link.springer.com/10.1007/BF00139635">Cliques, clubs and clans</a><br>Conference/Journal: Quality &amp; Quantity, 1979</p>
<h3 id="2-2-3-R-clan"><a href="#2-2-3-R-clan" class="headerlink" title="2.2.3 R-clan"></a>2.2.3 R-clan</h3><p>Paper: <a href="http://link.springer.com/10.1007/BF00139635">Cliques, clubs and clans</a><br>Conference/Journal: Quality &amp; Quantity, 1979</p>
<h2 id="2-3-Connectivity-based-relaxed-clique-models"><a href="#2-3-Connectivity-based-relaxed-clique-models" class="headerlink" title="2.3 Connectivity-based relaxed clique models"></a>2.3 Connectivity-based relaxed clique models</h2><h3 id="2-3-1-K-VCC-k-block"><a href="#2-3-1-K-VCC-k-block" class="headerlink" title="2.3.1 K-VCC (k-block)"></a>2.3.1 K-VCC (k-block)</h3><p>Paper1: <a href="https://epubs.siam.org/doi/10.1137/0204043">Network Flow and Testing Graph Connectivity</a><br>Conference/Journal: SIAM Journal on Computing, 1975<br>Paper2: <a href="https://linkinghub.elsevier.com/retrieve/pii/0095895678900710">k-Blocks and ultrablocks in graphs</a><br>Conference/Journal: Journal of Combinatorial Theory, Series B, 1978</p>
<h3 id="2-3-2-K-bundle"><a href="#2-3-2-K-bundle" class="headerlink" title="2.3.2 K-bundle"></a>2.3.2 K-bundle</h3><p>Paper: <a href="https://www.sciencedirect.com/science/article/pii/S0377221712007679">On clique relaxation models in network analysis</a><br>Conference/Journal: European Journal of Operational Research, 2013</p>
<h3 id="2-3-3-K-ECC"><a href="#2-3-3-K-ECC" class="headerlink" title="2.3.3 K-ECC"></a>2.3.3 K-ECC</h3><p>Paper: <a href="https://doi.org/10.1145/2983323.2983748">Querying Minimal Steiner Maximum-Connected Subgraphs in Large Graphs</a><br>Conference/Journal: CIKM, 2016</p>
<h2 id="2-4-Edge-based-relaxed-clique-models"><a href="#2-4-Edge-based-relaxed-clique-models" class="headerlink" title="2.4 Edge-based relaxed clique models"></a>2.4 Edge-based relaxed clique models</h2><h3 id="2-4-1-K-defective-clique"><a href="#2-4-1-K-defective-clique" class="headerlink" title="2.4.1 K-defective-clique"></a>2.4.1 K-defective-clique</h3><p>Paper: <a href="https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btl014">Predicting interactions in protein networks by completing defective cliques</a><br>Conference/Journal: Bioinformatics, 2006</p>
<h3 id="2-4-2-gamma-quasi-clique"><a href="#2-4-2-gamma-quasi-clique" class="headerlink" title="2.4.2 $\gamma$-quasi-clique"></a>2.4.2 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex;" xmlns="http://www.w3.org/2000/svg" width="1.229ex" height="1.486ex" role="img" focusable="false" viewBox="0 -441 543 657"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FE" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z"></path></g></g></g></svg></mjx-container>-quasi-clique</h3><p>Paper1: <a href="https://doi.org/10.1145/1081870.1081898">On mining cross-graph quasi-cliques</a><br>Conference/Journal: KDD, 2005<br>Paper2: <a href="https://doi.org/10.1145/1150402.1150506">Coherent closed quasi-clique discovery from large dense graph databases</a><br>Conference/Journal: KDD, 2006</p>
<p>##2.5 Triangulation-based relaxed clique models</p>
<h3 id="2-5-1-DN-graph"><a href="#2-5-1-DN-graph" class="headerlink" title="2.5.1 DN-graph"></a>2.5.1 DN-graph</h3><p>Paper: <a href="https://doi.org/10.14778/1921071.1921073">On triangulation-based dense neighborhood graph discovery</a><br>Conference/Journal: Proceedings of the VLDB Endowment, 2010</p>
<h3 id="2-5-2-K-truss"><a href="#2-5-2-K-truss" class="headerlink" title="2.5.2 K-truss"></a>2.5.2 K-truss</h3><p>Paper: <a href="https://dl.acm.org/doi/10.14778/2311906.2311909">Truss decomposition in massive networks</a><br>Conference/Journal: Proceedings of the VLDB Endowment, 2012</p>
]]></content>
      <categories>
        <category>Collection</category>
      </categories>
      <tags>
        <tag>Collection</tag>
        <tag>Cohesive Subgraph</tag>
      </tags>
  </entry>
  <entry>
    <title>Mobile3DRecon</title>
    <url>/papernote/Mobile3DRecon/</url>
    <content><![CDATA[<p>对论文 Mobile3DRecon:Real-time Monocular 3D Reconstruction on a Mobile Phone (TVCG, 2020) 的阅读整理。</p>
<span id="more"></span>

<center><b><font size=9>Mobile3DRecon</font></b></center>
# 论文情况
- 标题：Mobile3DRecon:Real-time Monocular 3D Reconstruction on a Mobile Phone
- 作者：Xingbin Yang, Liyang Zhou, Hanqing Jiang, and etc.
- 会议：IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS (TVCG) 2020
- 源码：（未开源，官方网站）https://zju3dv.github.io/mobile3drecon.github.io/



<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1  Introduction"></a>1  Introduction</h1><p>现有的一些基于 6DoF 惯导系统的手机 AR 技术如 VINS-Mobile 主要关注于不考虑场景中深度几何结构的 sparse mapping，而深度几何结构对于场景的全面 3D 感知是关键的，并且对于更加真实的 AR（遮挡、阴影、碰撞）是十分重要的。KinectFusion、BundleFusion 等尝试使用 dense mapping 来执行 6DoF 惯导，但依赖于通过 RGB-D 相机拍摄的带深度信息的视频作为输入，同时 dense mapping 需要大量的运算，这对于大多数中低档手机来说要求过高。<br>为使 AR 技术能更适用于中档手机平台，本文提出了一个面向中档手机的深度表面重建系统 Mobile3DRecon。系统可以实现以单目相机实时表面网格重建（surface mesh reconstruction），无需额外的硬件或深度传感器。<br>主要贡献：</p>
<ul>
<li>提出了一个多视觉关键帧深度估计的方法，甚至可以在结构不清晰的区域估计稠密的深度信息。通过基于置信度的过滤方法提出 SGM（semi-global matching）来可靠地估计深度和移除因位姿错误或结构不清晰而导致的不可靠深度信息。</li>
<li>提出了一个增强的网格生成方法，融合了计算得到的关键帧深度图来在线重建场景表面网格。</li>
<li>提出了一个使用单目相机的实时稠密表面网格（dense surface mesh）重建方法。在中档手机的实验上，所提出的方法后端处理速度不超过 125 ms&#x2F;keyframe，足够匹配前端 6DoF 追踪醉倒 25 FPS 的速度。</li>
</ul>
<h1 id="2-System-Overview"><a href="#2-System-Overview" class="headerlink" title="2  System Overview"></a>2  System Overview</h1><center>
    <img src="./Mobile3DRecon/1 系统框架.png">
    <div>
        图-1  系统框架
    </div>
</center>


<p>当用户使用手机单目相机进入环境当中，系统使用 SenseAR SLAM[1] 来进行位姿追踪（可以使用 AR Core)。</p>
<p>当前端 6DoF 的追踪初始化完成后，对于新到来的 keyframe 以及其全局优化后的位姿，其深度图通过多视觉 SGM 在线估计得到。在 SGM 后面是一个 CNN 来过滤处理深度噪声（depth noise）。经过处理后的 keyframe depth map 将被融合生成稠密表面网格。</p>
<p>如上图，back end 的处理包括了全局位姿优化、SGM、CNN 以及表面网格生成等。</p>
<h1 id="3-Monocular-Depth-Estimation"><a href="#3-Monocular-Depth-Estimation" class="headerlink" title="3  Monocular Depth Estimation"></a>3  Monocular Depth Estimation</h1><p>使用从 SenseAR SLAM 平台获取到的全局优化 keyframe 位姿，可以实现在线估计每一个新到来 keyframe 的深度图。</p>
<p>本文的系统通过广义的多视觉 SGM 算法来解决多视觉深度估计问题，不同于广义的二视觉 SGM 算法[2, 3]，本文的 SGM 方法更加鲁棒，因为深度噪声可以通过多视觉损失的方式来减小。</p>
<p>此外，由于直接在深度空间（depth space）而不是视差空间（disparity space）计算损失，所以不需要提前进行立体校正。</p>
<p>同时，考虑到位姿错误、结构不清晰或者碰撞等问题导致的深度信息错误，使用基于 DNN 的优化方法来过滤和优化深度噪声。</p>
<h2 id="3-1-Reference-Keyframe-Selection"><a href="#3-1-Reference-Keyframe-Selection" class="headerlink" title="3.1  Reference Keyframe Selection"></a>3.1  Reference Keyframe Selection</h2><p>对于每一个到来的 keyframe，寻找一些邻居 keyframe 集合作为参考帧（reference frame），用于立体多视觉算法：</p>
<ul>
<li>首先，参考帧应该为稳定的深度估计提供提供足够的视差。因为对于 baseline 较小的情况下，一个小的图像域像素匹配误差会导致深度空间的较大波动。</li>
<li>其次，为实现全场景的重建，当前的 keyframe 应该与参考帧有足够大的重叠部分（overlap）。</li>
</ul>
<p>为满足上述需求，选取远离当前 keyframe 的邻居 keyframe，同时避免过大的 baseline 使得重叠部分太少。为简洁，使用 $t$ 表示 $t$ 时刻的 keyframe，使用如下的式子来表示 keyframe $t$ 和另一个 keyframe $t’$ 的 baseline 分数：<br>$$<br>w_b(t, t’) &#x3D; \exp (-{(b(t, t’) - b_m)^2 \over \delta^2})<br>\tag{1}<br>$$<br>其中 $b(t, t’)$ 为 $t$ 和 $t’$ 之间的 baseline，$b_m$ 为期望，$\delta$ 为标准差，实验中设置为 $b_m &#x3D; 0.6,\ \delta &#x3D; 0.2$。</p>
<p>为了更好地匹配，$t$ 和 $t’$ 之间应该有尽可能满足相同视觉感知的区域。本文通过计算两个 keyframe 视轴（optical axis）的角度，然后大体上，大角度说明有着不同的视觉感知区域。为保证大的 overlap，定义 $t$ 和 $t’$ 之间的视觉分数：<br>$$<br>w_v(t, t’) &#x3D; \max ({\alpha_m \over \alpha(t, t’)}, 1)<br>\tag{2}<br>$$<br>其中 $\alpha(t, t’)$ 为 $t$ 和 $t’$ 之间的视觉方向的夹角，$\alpha_m$ 为阈值并设置为 $10 \degree$。</p>
<p>打分函数则简单地将上述两式相乘：<br>$$<br>S(t, t’) &#x3D; w_b(t, t’) * w_v(t, t’)<br>\tag{3}<br>$$<br>对于每一个新的 keyframe，在历史 keyframes 中寻找其参考帧。这些历史 keyframes 通过上述打分函数进行打分并排序，分数高的几帧将作为参考帧。</p>
<p>较多的关键帧有助于确保深度估计的准确性，但是将降低立体匹配计算的速度，尤其对于手机。因此，仅选择排序后前 2 个 keyframe 为参考帧。对于新的 keyframe $t$，其最终分数（final score）为：<br>$$<br>S(t) &#x3D; \sum_{t’ \in N(t)} S(t, t’)<br>\tag{4}<br>$$<br>其中 $N(t)$ 为 $t$ 的参考帧集合。这个最终分数将被用于之后损失计算中的权重。</p>
<h2 id="3-2-Multi-View-Stereo-Matching"><a href="#3-2-Multi-View-Stereo-Matching" class="headerlink" title="3.2  Multi-View Stereo Matching"></a>3.2  Multi-View Stereo Matching</h2><p>对于每个新 keyframe，使用基于多视觉立体算法的 SGM 来计算其深度图。将逆深度空间均匀采样为 $L$ 个 level。假设深度测量界限于 $d_{min}$ 和 $d_{max}$ 之间，则第 $l$ 级采样深度为：<br>$$<br>d_l &#x3D; {(L - 1) d_{min} d_{max} \over (L - 1 - l) d_{min} + l (d_{max} - d_{min})},\ l \in { 0, 1, …, L-1 }<br>\tag{5}<br>$$<br>给定一个 keyframe $t$ 中深度为 $d_l$ 的像素 $\pmb{x}$，其投影到 keyframe $t’$ 在深度 $d_l$ 下对应的像素 $\pmb{x}<em>{t \rightarrow t’}(d_l)$ 可以通过下式计算：<br>$$<br>\begin{aligned}<br>\pmb{x}</em>{t \rightarrow t’} (d_l) &amp;\sim d_l \pmb{K}<em>{t’} \pmb{R}</em>{t’} \pmb{R}<em>t^T \pmb{K}<em>t^{-1} \hat{\pmb{x}} + \pmb{K}</em>{t’} (\pmb{T}</em>{t’} - \pmb{R}<em>{t’} \pmb{R}<em>t^T \pmb{T}<em>t)<br>\end{aligned}<br>\tag{6}<br>$$<br>其中 $\pmb{K}</em>{t’}, \pmb{K}</em>{t}$ 为相机内参矩阵，$\pmb{R}</em>{t’}, \pmb{R}<em>{t}$ 为旋转矩阵，$\pmb{T}</em>{t’}, \pmb{T}_{t}$ 为位移矩阵（向量），$\hat{\pmb{x}}$ 为 $\pmb{x}$ 的齐次坐标。</p>
<p>使用 Census Transform 的变体来计算局部图片的相似度。相比其他图片相似度计算方法（如 Normalized Cross Correlation），CT 具有边界保持的特点，同时 CT 的二进制计算有益于移动应用。本文将匹配损失定义为如下：<br>$$<br>C(\pmb{x}, l) &#x3D; \sum_{t’ \in N(t)} w_{t’} CT(\pmb{x}, \pmb{x}<em>{t \rightarrow t’}(d_l)) \<br>w</em>{t’} &#x3D; {S(t’) \over \sum_{\tau \in N(t)} S(\tau)}<br>\tag{7}<br>$$<br>$w_{t’}$ 为参考帧 $t’$ 的损失权重，$S(t’)$ 为 $t’$ 的 final score，$CT(\pmb{x}, \pmb{x}<em>{t \rightarrow t’}(d_l))$ 为两个以 $\pmb{x}$ 和 $\pmb{x}</em>{t \rightarrow t’}(d_l)$ 为中心的局部图片的 Census 损失。（提前设立 look-up-table 来加快 CT 过程中汉明距离的计算）</p>
<p>遍历每个像素计算其在 $l$ 层的匹配损失，得到一个大小为 $W \times H \times L$ 的损失矩阵，其中 $W$ 和 $H$ 为 keyframe 的宽和高。然后损失矩阵通过 Winner-Take-All 策略进行聚合获得初始 depth map。</p>
<p>尽管位姿误差可以在多视觉匹配过程中被稀释，但仍然存在歧义匹配的问题（区域不清晰等），将对 depth map 带来噪声。本文增加了一个额外的正则化方法惩罚像素的深度标签（采样 level）变化。特别地，对采样等级标签为 $l$ 的像素 $\pmb{x}$，损失融合通过在邻居方向中递归的进行损失计算得到：<br>$$<br>\hat{C}_r(\pmb{x}, l) &#x3D; C(\pmb{x}, l) + \min<br>\left(<br>\begin{matrix}<br>\hat{C}_r(\pmb{x} - r, l), \<br>\hat{C}_r(\pmb{x} - r, l - 1) + P_1, \<br>\hat{C}_r(\pmb{x} - r, l + 1) + P_1, \<br>\underset{i}{\min} \hat{C}_r(\pmb{x} - r, i) + P_2<br>\end{matrix}<br>\right)</p>
<ul>
<li>\min_k \hat{C}_r(\pmb{x} - r, k)<br>\tag{8}<br>$$<br>其中 $\hat{C}_r(\pmb{x}, l)$ 为标签 $l$ 的像素 $\pmb{x}$ 在邻居方向 $r \in N_r$ 下的聚合损失，邻居方向设置为 8 个方向。$P_1$ 和 $P_2$ 为惩罚值，其中 $P_2 &#x3D; -a * |\nabla I_t(\pmb{x})| + b$，$\nabla I_t(\pmb{x})$ 为 keyframe $t$ 对应图像 $I_t$ 的强度梯度。$\min_k \hat{C}<em>r(\pmb{x} - r, k)$ 用于避免聚合损失的无限增长。图像中每个像素的最终损失为：<br>$$<br>\hat{C}(\pmb{x}, l) &#x3D; \sum</em>{r \in N_r} \hat{C}_r (\pmb{x}, l)<br>\tag{9}<br>$$<br>最终的深度 level 标签则通过 Winner-Take-All 策略得到：<br>$$<br>\hat{l} (\pmb{x}) &#x3D; \underset{l}{\operatorname{argmin}} \hat{C} (\pmb{x}, l) \<br>(注：原文中为\ \hat{l} (\pmb{x}) &#x3D; \underset{l}{\operatorname{min}} \hat{C} (\pmb{x}, l)，感觉不符合此处对\ \hat{l} (\pmb{x})\ 的描述)<br>\tag{10}<br>$$<br>为了获得 sub-level 的深度值，先使用抛物线拟合的方法进行计算：<br>$$<br>\hat{l}_s(\pmb{x}) &#x3D; \hat{l}(\pmb{x}) + {\hat{C}(\pmb{x}, \hat{l}{(\pmb{x}) - 1}) - \hat{C}(\pmb{x}, \hat{l}{(\pmb{x}) + 1}) \over 2 \hat{C}(\pmb{x}, \hat{l}{(\pmb{x})-1}) - 4\hat{C}(\pmb{x}, \hat{l}{(\pmb{x})}) + 2\hat{C}(\pmb{x}, \hat{l}(\pmb{x})+1)}<br>\tag{11}<br>$$<br>用 $\hat{l}_s(\pmb{x})$ 替换上述式子 (5) 中的 $l$ 得到 $\pmb{x}$ 的 sub-level 深度。有了 sub-level 深度，可以得到一个初始的 keyframe $t$ 的深度图 $D_t^i$，如图-4(b)。</li>
</ul>
<p>实验中使用 NEON 技术来加速损失的计算和融合。</p>
<h2 id="3-3-Depth-Refinement"><a href="#3-3-Depth-Refinement" class="headerlink" title="3.3  Depth Refinement"></a>3.3  Depth Refinement</h2><p>尽管文中的方法已经比 SLAM 更加鲁棒，但是仍然存在深度噪声。</p>
<h3 id="3-3-1-Confidence-Based-Depth-Filtering"><a href="#3-3-1-Confidence-Based-Depth-Filtering" class="headerlink" title="3.3.1  Confidence-Based Depth Filtering"></a>3.3.1  Confidence-Based Depth Filtering</h3><p>尽管上述 SGM 方法获得的 depth map 已经很完整，但是在某些结构不清晰的区域仍然明显存在噪声，因此需要置信度测量来深化深度信息的过滤。在 Drory 等人的方法[4] 中，提出了为 SGM 方法计算像素 $\pmb{x}$ 的不确定性的方法：<br>$$<br>U(\pmb{x}) &#x3D; \min_l \sum_{r \in N_r} \hat{C}<em>r(\pmb{x}, l) - \sum</em>{r \in N_r} \min_l (\hat{C}_r (\pmb{x}, l) - {N_r - 1 \over N_r} C(\pmb{x}, l))<br>\tag{12}<br>$$<br>然而这种方法忽略了邻居的深度信息，造成了某些离散的深度噪声，如图-2 (d)。</p>
<center>
    <img src="./Mobile3DRecon/2 深度过滤结果.png">
    <div>
        图-2  深度信息过滤
    </div>
</center>

<p>考虑到正确的深度测量结果下，邻居像素的深度不会大幅度改变，因此为 $\pmb{x}$ 在其周围 $5\times5$ 的窗口 $\Omega(\pmb{x})$ 内计算一个权重 $\omega(\pmb{x})$，测量其邻居的深度 level 的不同：<br>$$<br>\omega (\pmb{x}) &#x3D; {1 \over |\Omega(\pmb{x})|} \sum_{\pmb{y} \in \Omega(\pmb{x})} [|\hat{l}_s(\pmb{x}) - \hat{l}_s(\pmb{y})| &gt; 2]<br>\tag{13}<br>$$<br>标准化加权的不确定性得到最终的置信度 $M(\pmb{x})$：<br>$$<br>M(\pmb{x}) &#x3D; 1 - {\omega(\pmb{x}) U(\pmb{x}) \over \underset{\pmb{u} \in I_t}{\max} (\omega(\pmb{u}) U(\pmb{u}))}<br>\tag{14}<br>$$<br>在实验中，将置信度低于 $0.15$ 的像素移除，得到过滤后的深度图 $D_t^f$，这样就使得大多数的 outliers 被移除，但还是会留下小部分的噪声，如图-4 (c)。</p>
<h3 id="3-3-2-DNN-Based-Depth-Refinement"><a href="#3-3-2-DNN-Based-Depth-Refinement" class="headerlink" title="3.3.2  DNN-Based Depth Refinement"></a>3.3.2  DNN-Based Depth Refinement</h3><p>在经过基于置信度的深度融合后，使用一个 DNN 来进一步过滤深度噪声。</p>
<center>
    <img src="./Mobile3DRecon/3 DNN.png">
    <div>
        图-3  DNN 框架
    </div>
</center>

<p>这里的 DNN 可以是为 two-stage 的结构：</p>
<ul>
<li>第一阶段：$CNN_G$ 结合 keyframe $t$ 的过滤后深度图 $D_t^f$ 和对应的灰度图 $I_t$ 得到一个粗糙的 refinement 结果 $D_t^c$。此处 $I_t$ 作为一个引导者来引导深度 refinement，其提供了对象边缘和语义信息的先验知识。</li>
<li>第二阶段：残差 U-Net $CNN_R$ 进一步优化 $D_t^c$ 得到最终的深度图 $D_t$。U-Net 的主要用于是学习过程更加稳定。</li>
</ul>
<p>引入三种空间损失函数[5, 6, 7] 来惩罚深度噪声同时保留边缘信息，训练的损失函数定义为：<br>$$<br>\Phi &#x3D; \Phi_{edge} + \Phi_{pd} + \lambda \Phi_{vgg}<br>\tag{15}<br>$$<br>$\Phi_{edge}$ 为边缘维护损失，包含三个组成部分（$\Phi_x$ 和 $\Phi_y$ 计算的是沿着边缘的深度差异，$\Phi_n$ 为法向量的相似度）：<br>$$<br>\Phi_{edge} &#x3D; \Phi_x + \Phi_y + \Phi_n \<br>\begin{aligned}<br>\Phi_x &amp;&#x3D; {1 \over |I_t|} \sum_{\pmb{x} \in I_t} \ln (|\nabla_x D_t(\pmb{x}) - \nabla_x D_t^g(\pmb{x})| + 1) \<br>\Phi_y &amp;&#x3D; {1 \over |I_t|} \sum_{\pmb{x} \in I_t} \ln (|\nabla_y D_t(\pmb{x}) - \nabla_y D_t^g(\pmb{x})| + 1) \<br>\Phi_n &amp;&#x3D; {1 \over |I_t|} \sum_{\pmb{x} \in I_t} (1 - \eta_t (\pmb{x}) * \eta_t^g (\pmb{x}))<br>\end{aligned}<br>\tag{16}<br>$$<br>此处 $|I_t|$ 为 $I_t$ 中还保留着的像素点数，$D_t^g(\pmb{x})$ 表示像素 $\pmb{x}$ 的 ground truth 深度值。$\nabla_x$ 和 $\nabla_y$ 代表沿 x 轴和 y 轴的 Sobel 导数。深度法向量 $\eta_t(\pmb{x})$ 近似为 $\eta_t (\pmb{x}) &#x3D; [- \nabla_x D_t(\pmb{x}), \nabla_y D_t(\pmb{x}), 1]$，$\eta_t^g(\pmb{x})$ 则为对应的通过相同方法计算得到的 ground truth 法向量。</p>
<p>$\Phi_{pd}$ 为光度计损失（photometric loss）：<br>$$<br>\Phi_{pd} &#x3D; {1 \over |I_t|} \sum_{\pmb{x} \in I_t} (|\nabla_x^2 D_t(\pmb{x})| e^{- \alpha |\nabla_x I_t(\pmb{x})|} + |\nabla_y^2 D_t(\pmb{x})| e^{- \alpha |\nabla_y I_t(\pmb{x})|})<br>\tag{17}<br>$$<br>其中实验中取 $\alpha &#x3D; 0.5$，$\nabla_x^2$ 和 $\nabla_y^2$ 为沿 x 轴和 y 轴的二阶导。使用二阶导的目的是让 refinement 过程更加稳定。</p>
<p>$\Phi_{vgg}$ 为 high-level perceptual loss[7]，通过最小化高阶层特征之间的差异来维护全局数据分布。 </p>
<p>实验在 Demon 数据集上进行。首先在 Demon 数据集上经过 SGM 和基于置信度的过滤方法生成一系列初始的过滤后 depth maps，然后将其与 ground truth 融合进行预训练。然后与训练后的模型将在作者用 OPPO R17 Pro 采集的数据集上进行深度训练。</p>
<p>如图-4 (d) 为经过 DNN 训练和计算后得到的深度图结果。</p>
<center>
    <img src="./Mobile3DRecon/4 单眼相机深度估计结果.png">
    <div>
        图-4  单目相机深度测量结果
    </div>
</center>


<h1 id="4-Incremental-Mesh-Generation"><a href="#4-Incremental-Mesh-Generation" class="headerlink" title="4  Incremental Mesh Generation"></a>4  Incremental Mesh Generation</h1><p>估计的深度将被融合仿真生成在线的 surface mesh。</p>
<p>TSDF（truncated signed distance function）在用于实时的网格重建。</p>
<p>本文中的 mesh 生成方法执行可扩展的 TSDF 体素融合，以避免体素哈希冲突，同时根据新融合体素的 TSDF 变化增量更新表 surface mesh。通过这种增量网格更新来在一个 CPU 核的情况下在手机上完成实时网格扩展。</p>
<h2 id="4-1-Scalable-Voxel-Fusion"><a href="#4-1-Scalable-Voxel-Fusion" class="headerlink" title="4.1  Scalable Voxel Fusion"></a>4.1  Scalable Voxel Fusion</h2><p>传统的 TSDF 虽然简洁，但是计算复杂并且消耗内存，不利于手机上实时的大场景重建。体素哈希被证明由于其低内存消耗和动态体素分配的方式，对于大型场景的重建是更具扩展性的，但仍然需要解决由哈希函数带来的哈希冲突问题。</p>
<p>受这些的启发，本文提出了结合传统 TSDF 体素索引和体素哈希优点的可扩展哈希算法。</p>
<h3 id="4-1-1-Scalable-Hash-Function"><a href="#4-1-1-Scalable-Hash-Function" class="headerlink" title="4.1.1  Scalable Hash Function"></a>4.1.1  Scalable Hash Function</h3><center>
    <img src="./Mobile3DRecon/5 TSDF cube.png">
    <div>
        图-5  TSDF Cube
    </div>
</center>

<p>如图-5 (a) 为 TSDF 算法所提取出的 cube 和体素。在本文提出的算法中，每一个 cube 及其相关的体素都能被分配一个唯一的编码。</p>
<p>假设有一个 3D 容器，预定义的大小为 $\gamma$，每一维的范围为 $[-\gamma &#x2F; 2, \gamma &#x2F; 2)$。$G &#x3D; \gamma &#x2F; \delta$ 为每一维的体素数量，其中 $\delta$ 为体素的大小（如 $0.06 \mathrm{m}$）。对于 volume 中的一个 3D 点 $\pmb{V} &#x3D; (fx, fy, fz)$，其所属的 cube 可以由以下的哈希函数索引到：<br>$$<br>h(x, y, z) &#x3D; g(x) + g(y) * G + g(z) * G^2 \<br>(x, y, z) &#x3D; (\lfloor {fx \over \delta} \rfloor, \lfloor {fy \over \delta} \rfloor, \lfloor {fz \over \delta} \rfloor)<br>\tag{18}<br>$$<br>其中：<br>$$<br>g(i) &#x3D; i + {G \over 2},\ i \in [-{G \over 2}, {G \over 2}) \<br>g: [-{G \over 2}, {G \over 2}) \mapsto [0, G)<br>$$<br>式子 (18) 可以将一个 volume 内的 3D 点分配到一个唯一索引，然而当一个 3D 点不在 volume 内时，如图-5 (b)，将出现冲突：</p>
<ul>
<li>假设 $G &#x3D; 5$，点 $\pmb{V}_a$ 在 volume 中有坐标 $(g(x), g(y), g(z)) &#x3D; (1, 1, 0)$，此时得到哈希值 $6$；若点 $\pmb{V}_b$ 在 volume 中计算得到的坐标为 $(6, 0, 0)$，则也会得到同样的哈希值，从而产生冲突。</li>
</ul>
<p>为解决上述冲突，此处重新定义哈希函数：<br>$$<br>\hat{h}(x, y, z) &#x3D; O_C + \hat{g}(x) + \hat{g}(y) * G + \hat{g}(z) * G^2 \<br>\hat{g}(i) &#x3D; i + {G O_G \over 2}<br>\tag{19}<br>$$<br>其中 $O_G$ 为较大的体素维度索引的偏移，$O_C$ 为全局偏移：<br>$$<br>O_G &#x3D;<br>\begin{cases}<br>\lfloor {2 \hat{i} \over G} \rfloor + 1, &amp;\hat{i} &gt; 0 \<br>\lfloor {- (2 \hat{i} + 1) \over G} \rfloor + 1, &amp;\hat{i} \le 0<br>\end{cases}<br>,\ \hat{i} &#x3D; \underset{i \in { x, y, z }}{\operatorname{argmax}} |i| \<br>O_C &#x3D; G^3 \sum_{k &#x3D; 1}^{O_G - 1} k^3<br>\tag{20}<br>$$</p>
<ul>
<li>有了式子 (19)，$\pmb{V}_b$ 重新计算得到哈希值为 $286$。</li>
</ul>
<p>此处的哈希改进是将索引范围从 $[0, G^3)$ 扩大到了 $[0, (GO_G)^3)$。这种改进扩大了哈希索引范围，避免了两个随机点产生的哈希冲突。</p>
<h3 id="4-1-2-Voxel-Fusion-with-Dynamic-Objects-Removal"><a href="#4-1-2-Voxel-Fusion-with-Dynamic-Objects-Removal" class="headerlink" title="4.1.2  Voxel Fusion with Dynamic Objects Removal"></a>4.1.2  Voxel Fusion with Dynamic Objects Removal</h3><p>假设有 $t$ 时刻的 depth map $D_t$，对于像素 $\pmb{x} &#x3D; (u, v)$ 的深度值 $d \in D_t$，将其投影到 3D 空间得到一个 3D 点 $P &#x3D; \pmb{M}_t^{-1} \rho(u, v, d)$，其中 $\rho(u, v, d) &#x3D; ({u - c_u \over f_u} d, {v- c_v \over f_v} d, d)$。$(f_u, f_v)$ 为在 u 和 v 方向点焦距长度，$(c_u, c_v)$ 为 2D 图像的中心，$\pmb{M}_t$ 为将全局 3D 空间转换到局部相机空间的转换矩阵。点 $P$ 所在的 cube 的哈希索引由 (19) 式得到。</p>
<p>如图-5，每个 cube 有 8 个关联体素，每个体素 $\pmb{V}$ 在第一次遍历到或者被更新时，将作为下式的新输入值：<br>$$<br>T_t (\pmb{V}) &#x3D; T_{t-1} (\pmb{V}) + d(\pmb{M}_t \pmb{V}) - D_t (\pi (\pmb{M}<em>t \pmb{V})) \<br>W_t (\pmb{V}) &#x3D; W</em>{t-1} (\pmb{V}) + 1<br>\tag{21}<br>$$<br>其中 $\pi(x, y, z) &#x3D; ({x \over z}f_u + c_u, {y \over z}f_v + c_v)$ 为投影函数，为简洁，记 $\pi(x, y, z) &#x3D; \pmb{x}$；$d(\pmb{M}_t \pmb{V})$ 为 keyframe $t$ 下体素 $\pmb{V}$ 在局部相机空间的投影深度；$D(\pmb{x})$ 为像素 $\pmb{x}$ 的深度测量值；$T_t(\pmb{V})$ 和 $W_t(\pmb{V})$ 为体素 $\pmb{V}$ 在时间 $t$ 的 TSDF 值和权重。对于新生成的体素，有：<br>$$<br>T_t (\pmb{V}) &#x3D; d(\pmb{M}_t \pmb{V}) - D_t (\pi (\pmb{M}_t \pmb{V})) \<br>W_t (\pmb{V}) &#x3D; 1<br>$$<br>有了 (21) 中的体素更新方法，可以实时地更新和生成被深度测量所占据的 cube 周围的体素。特别地，维护一个 cube 列表来记录所有生成的 cube。对于列表中的每一个 cube，与其相关联的体素哈希也被记录下来，使得两个相邻的 cube 可以用相同的哈希来共享体素。</p>
<p>使用 Marching cubes 算法从 cube 列表里提取出等值面（iso-surface）。注意到如果任何和 cube 相关联的 TSDF 体素被投影到 depth map 边界外或者投影到错误的深度像素中，则和这个 cube 相关联的所有体素的更新都需要还原。如果一个 cube 的 8 个体素在 TSDF 更新后值均小于 0，则这个 cube 需要被移除。</p>
<center>
    <img src="./Mobile3DRecon/6 动态物体移除.png">
    <div>
        图-6  动态物体移除
    </div>
</center>

<p>如图-6，现实的 AR 场景重建中会遇到画面中存在动态物体的情况，尤其是画面中的物体出现后停留片刻又离开，将对场景重建带来极大的困难。</p>
<p>除了更新与当前估计深度测量相关联的体素外，还将每个现有体素 $\pmb{V}$ 投射到当前帧 $t$，以进行深度可见性检查：</p>
<ul>
<li>如果 $d(\pmb{M}_t \pmb{V}) &lt; D_t(\pmb{x})$，即深度测量结果在模型的前侧，此时出现了可见性冲突。这在一个物体进入相机视觉内停留了前几帧后，而在后续几帧中又消失的情况下发生。体素融合了物体保持静止的前几帧，在之后几帧中物体消失了，这就引发了冲突。解决这种情况的方式就是深入的使用式子 (21) 更新体素 $\pmb{V}$ 的 TSDF 值。</li>
<li>如果 $d(\pmb{M}_t \pmb{V}) \ll D_t(\pmb{x})$，TSDF 值将很快下降至小于 0，使得其关联的 cube 被无效化。这种策略使得被动态物体所占据的 cube 很快被移除。</li>
</ul>
<p>使用这些策略，移动物体将被逐渐移除，重建出 surface mesh。</p>
<h2 id="4-2-Incremental-Mesh-Updating"><a href="#4-2-Incremental-Mesh-Updating" class="headerlink" title="4.2  Incremental Mesh Updating"></a>4.2  Incremental Mesh Updating</h2><p>Marching cube 是一个从 TSDF 值中提取 iso-surface 的有效算法，被广泛用于一些深度重建系统。然而，由于频繁的插值操作导致的性能问题，这些系统大多在实时重建完成后作为后处理进行表面提取。大多数 AR 应用需要实时的网格生成来支持增量更新。本文设计了一个增量网格更新策略，仅占用 1 个 CPU 线程，避免前端对 GPU 资源的占用。</p>
<p>这里的增量更新策略是基于每个 keyframe 中需要被更新的 cubes，iso-surface 则从这些 cube 中生成。为得到那些参与表面提取的 cubes，设每个体素有状态变量 $\chi(\pmb{V}) \in { ADD, UPDATE, NORMAL, DELETE }$：</p>
<ul>
<li>如果 $\pmb{V}$ 为新分配的体素，则其状态设为 $ADD$；</li>
<li>如果 $\pmb{V}$ 的 TSDF 值 $T_t(\pmb{V})$ 在 $t$ 发生更新，则状态设为 $UPDATE$；</li>
<li>如果 $T_t(\pmb{V}) \le 0$ 或者 $W_t(\pmb{V}) \le 0$，则状态设为 $DELETE$，也就是说  $\pmb{V}$ 需要从当前的体素列表中移出，然后移入空体素列表并等待重新分配。</li>
</ul>
<p>一个 cube 的至少一个关联体素的状态为 $UPDATE$ 或 $ADD$ 时，定义这个 cube 为 updated。在经过 4.1.2 中的体素更新后，从 updated cube 提取网格三角（mesh triangles），如果提取的网格三角已经存在，则移除这些重复的三角。当网格三角提取完成后，这些体素的状态被设为 $NORMAL$。</p>
<p>如果一个 cube 的一个关联体素状态为 $DELETE$，则定义这个 cube 为 deleted，由其提取的网格三角也需要移除。</p>
<p>如图-7 为增量网格更新的过程：</p>
<center>
    <img src="./Mobile3DRecon/7 增量网格更新.png">
    <div>
        图-7  增量网格更新
    </div>
</center>

<p>如图-8 为四种场景下表面网格重建的结果：</p>
<center>
    <img src="./Mobile3DRecon/8 网格重建结果.png">
    <div>
        图-8  网格重建结果
    </div>
</center>




<h1 id="5-Experiment"><a href="#5-Experiment" class="headerlink" title="5  Experiment"></a>5  Experiment</h1><h2 id="5-1-实验设置"><a href="#5-1-实验设置" class="headerlink" title="5.1  实验设置"></a>5.1  实验设置</h2><ul>
<li>实验中使用的中档手机型号：OPPO R17 Pro</li>
<li>实验代码及第三方库：C++，OpenCV 2，Eigen 3</li>
</ul>
<h2 id="5-2-定量和定性实验结果"><a href="#5-2-定量和定性实验结果" class="headerlink" title="5.2  定量和定性实验结果"></a>5.2  定量和定性实验结果</h2><ul>
<li>depth map 重建结果对比：</li>
</ul>
<center>
    <img src="./Mobile3DRecon/9 深度图重建对比.png">
    <div>
        图-9  深度图重建对比
    </div>
</center>

<center>
    <img src="./Mobile3DRecon/10 深度图重建对比2.png">
    <div>
        图-10  深度图重建对比 2
    </div>
</center>

<ul>
<li>实验结果的准确度：</li>
</ul>
<center>
    <img src="./Mobile3DRecon/11 准确度.png">
    <div>
        图-11  实验结果误差
    </div>
</center>

<ul>
<li>处理时间：</li>
</ul>
<center>
    <img src="./Mobile3DRecon/12 处理时间.png">
    <div>
        图-12  各项计算的处理时间
    </div>
</center>



<h2 id="5-2-AR-应用"><a href="#5-2-AR-应用" class="headerlink" title="5.2  AR 应用"></a>5.2  AR 应用</h2><ul>
<li>Unity mobile 平台上实现 AR 效果，如 occlusion 和 collision：</li>
</ul>
<center>
    <img src="./Mobile3DRecon/13 Unity效果.png">
    <div>
        图-13  Unity 上的应用效果
    </div>
</center>



<h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="1-Census-Transform（CT）"><a href="#1-Census-Transform（CT）" class="headerlink" title="1  Census Transform（CT）"></a>1  Census Transform（CT）</h2><h3 id="1-1-立体匹配算法"><a href="#1-1-立体匹配算法" class="headerlink" title="1.1 立体匹配算法"></a>1.1 立体匹配算法</h3><p>主要分为两大类：</p>
<ul>
<li><p>基于全局约束：在本质上属于优化算法，它是将立体匹配问题转化为寻找全局能量函数的最优化问题，其代表算法主要有图割算法、置信度传播算法和协同优化算法等。全局算法能够获得较低的总误匹配率，但算法复杂度较高，很难满足实时的需求，不利于在实际工程中使用。</p>
</li>
<li><p>基于局部约束：主要是利用匹配点周围的局部信息进行计算，由于其涉及到的信息量较少，匹配时间较短，因此受到了广泛关注，代表算法主要有 SAD、SSD、NCC 等。</p>
</li>
</ul>
<h3 id="1-2-基于局部约束的立体匹配算法"><a href="#1-2-基于局部约束的立体匹配算法" class="headerlink" title="1.2  基于局部约束的立体匹配算法"></a>1.2  基于局部约束的立体匹配算法</h3><p>Census Transform（CT）：在实际场景中，造成亮度差异的原因有很多，如：由于左右摄像机不同的视角接受到的光强不一致、摄像机增益、电平存在差异，以及图像采集不同通道的噪声不同等。Cencus 方法保留了窗口中像素的位置特征，并且对亮度偏差较为鲁棒，简单讲就是能够减少光照差异引起的误匹配。</p>
<p>实现原理：</p>
<ol>
<li>在视图中选取任一点，以该点为中心划出一个例如 $3 × 3$ 的矩形。</li>
<li>矩形中除中心点之外的每一点都与中心点进行比较，灰度值小于中心点则记为1，灰度大于中心点则记为0，以所得长度为 8 的只有 0 和 1 的序列作为该中心点的 census 序列，即中心像素的灰度值被 census 序列替换。</li>
<li>经过 CT 后的图像使用汉明距离计算相似度，图像匹配就是在视差图中找出与参考像素点相似度最高的点，而汉明距离正是视差图像素与参考像素相似度的度量。具体而言，对于欲求取视差的左右视图，要比较两个视图中两点的相似度，可将此两点的 census 值逐位进行异或运算，然后计算结果为1 的个数，记为此两点之间的汉明值。</li>
</ol>
<p>汉明值是两点间相似度的一种体现，汉明距越小即相似度越高。</p>
<h3 id="1-3-代码"><a href="#1-3-代码" class="headerlink" title="1.3  代码"></a>1.3  代码</h3><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cv;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">CensusTransform</span><span class="params">(Mat input_image, Mat&amp; modified_image, <span class="type">int</span> window_sizex, <span class="type">int</span> window_sizey)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> image_height = input_image.rows;</span><br><span class="line">    <span class="type">int</span> image_width = input_image.cols;</span><br><span class="line"></span><br><span class="line">    modified_image = Mat::<span class="built_in">zeros</span>(image_height, image_width, CV_64F);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Census Transform</span></span><br><span class="line">    <span class="type">int</span> offsetx = (window_sizex - <span class="number">1</span>) / <span class="number">2</span>;</span><br><span class="line">    <span class="type">int</span> offsety = (window_sizey - <span class="number">1</span>) / <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> j = <span class="number">0</span>; j &lt; image_width - window_sizex; j++) &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; image_height - window_sizey; i++) &#123;</span><br><span class="line">            <span class="type">unsigned</span> <span class="type">long</span> census = <span class="number">0</span>;</span><br><span class="line">            <span class="comment">// 窗口中心像素</span></span><br><span class="line">            uchar current_pixel = input_image.<span class="built_in">at</span>&lt;uchar&gt;(i + offsety, j + offsetx); </span><br><span class="line">            <span class="comment">// 方形窗口</span></span><br><span class="line">            <span class="function">Rect <span class="title">roi</span><span class="params">(j, i, window_sizex, window_sizey)</span></span>; </span><br><span class="line">            <span class="function">Mat <span class="title">window</span><span class="params">(input_image, roi)</span></span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> a = <span class="number">0</span>; a &lt; window_sizey; a++) &#123;</span><br><span class="line">                <span class="keyword">for</span>(<span class="type">int</span> b = <span class="number">0</span>; b &lt; window_sizex; b++)  &#123;</span><br><span class="line">                    <span class="keyword">if</span> (!(a == offsety &amp;&amp; b == offsetx)) &#123;  <span class="comment">// 中心像素不做判断</span></span><br><span class="line">                        census = census &lt;&lt; <span class="number">1</span>; <span class="comment">// 左移1位</span></span><br><span class="line">                    &#125;</span><br><span class="line">                    uchar temp_value = window.<span class="built_in">at</span>&lt;uchar&gt;(a, b);</span><br><span class="line">                    <span class="keyword">if</span> (temp_value &lt;= current_pixel ) &#123; <span class="comment">// 当前像素小于中心像素</span></span><br><span class="line">                        census += <span class="number">1</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            modified_image.<span class="built_in">at</span>&lt;<span class="type">double</span>&gt;(i + offsety, j + offsetx) = census;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="1-4-参考"><a href="#1-4-参考" class="headerlink" title="1.4  参考"></a>1.4  参考</h3><ul>
<li><a href="https://blog.csdn.net/weixin_41874599/article/details/84071376">https://blog.csdn.net/weixin_41874599/article/details/84071376</a></li>
<li><a href="https://www.cnblogs.com/riddick/p/7295581.html">https://www.cnblogs.com/riddick/p/7295581.html</a></li>
</ul>
<h2 id="2-Winner-Take-All（胜者为王）"><a href="#2-Winner-Take-All（胜者为王）" class="headerlink" title="2  Winner-Take-All（胜者为王）"></a>2  Winner-Take-All（胜者为王）</h2><h3 id="2-1-聚类任务"><a href="#2-1-聚类任务" class="headerlink" title="2.1  聚类任务"></a>2.1  聚类任务</h3><p>任务中没有给出每个类别的标准特征，需要考察样本间的相似情况，将相似的样本聚为一类，此外还要找到每类的标准特征，即<strong>聚类中心</strong>。</p>
<p>比较两个样本的相似性可转化为测量两个样本点在样本空间的距离，并以此距离的大小作为两样本相似性的度量。</p>
<p>虽然从 2D 和 3D 样本空间的图中可以十分清晰地看出样本间的距离远近，从而可以通过目测发现隐藏在 2D 或 3D 样本空间中的聚类，但是随着样本空间维数的增加，很难再通过目测来进行观察。</p>
<p>一般来说，需要用某种合适的聚类算法来解决具体问题。</p>
<h3 id="2-2-最简单的聚类算法——胜者为王"><a href="#2-2-最简单的聚类算法——胜者为王" class="headerlink" title="2.2  最简单的聚类算法——胜者为王"></a>2.2  最简单的聚类算法——胜者为王</h3><p>该算法可以分为三个步骤：</p>
<ol>
<li>初始化聚类中心：拟将样本聚为多少类，就需要设置多少个权向量，每个权向量代表某一类的聚类中心。聚类中心的初始化方法很多，可以用随机数为每个向量赋值，也可以随机指定某个样本。</li>
<li>确定竞争获胜的权向量：从训练集中随机且不重复的抽取样本为算法的输入，每个输入样本均与权向量进行相似性比较，即测量当前输入样本在样本空间的位置与各个权向量在样本空间的欧氏距离，与当前输入样本欧氏距离最短的权向量在相似性竞争中获胜。</li>
</ol>
<center>
    <img src="./Mobile3DRecon/附-1 获胜权向量.png">
    <div>
        图-附 1  获胜权向量
    </div>
</center>


<ol start="3">
<li>获胜权向量的权值调整：胜者为王算法规定只有获胜权向量才可进行权值调整。调整方法是：令权向量想当前输入样本方向移动一步，步长与两点间的距离成正比。比例系数 $\alpha \in (0. 1]$ 为学习率。</li>
</ol>
<center>
    <img src="./Mobile3DRecon/附-2 权向量调整.png">
    <div>
        图-附 2  权向量调整
    </div>
</center>


<ol start="4">
<li>返回 2 输入下一个样本，直到训练至设定的训练次数。</li>
</ol>
<h3 id="2-3-参考"><a href="#2-3-参考" class="headerlink" title="2.3  参考"></a>2.3  参考</h3><ul>
<li><a href="https://baijiahao.baidu.com/s?id=1664391142733714225">https://baijiahao.baidu.com/s?id=1664391142733714225</a></li>
</ul>
<h2 id="3-Sobel-算子"><a href="#3-Sobel-算子" class="headerlink" title="3  Sobel 算子"></a>3  Sobel 算子</h2><h3 id="3-1-Sobel-算子"><a href="#3-1-Sobel-算子" class="headerlink" title="3.1  Sobel 算子"></a>3.1  Sobel 算子</h3><p>Sobel 算子：是离散微分算子（discrete differentiation operator），用来计算图像灰度的近似梯度，梯度越大越有可能是边缘。</p>
<p>Soble 算子的功能集合了高斯平滑和微分求导，又被称为一阶微分算子，求导算子，在水平和垂直两个方向上求导，得到的是图像在 X 方向与 Y 方向梯度图像。</p>
<p>缺点：比较敏感，容易受影响，要通过高斯模糊（平滑）来降噪。</p>
<h3 id="3-2-计算方法"><a href="#3-2-计算方法" class="headerlink" title="3.2  计算方法"></a>3.2  计算方法</h3><p>假设被作用图像为 $I$：</p>
<ul>
<li><p>水平变化：将 $I$ 与一个基数大小的内核进行卷积。如当内核大小为 3 时，X 轴方向的梯度 $G_x$ 的计算结果为：<br>$$<br>G_x &#x3D;<br>\left[<br>\begin{matrix}<br>-1 &amp;0 &amp;+1 \<br>-2 &amp;0 &amp;+2 \<br>-1 &amp;0 &amp;+1<br>\end{matrix}<br>\right]</p>
<ul>
<li>I<br>$$</li>
</ul>
</li>
<li><p>竖直变化：类比水平变化。Y 轴方向的梯度 $G_y$ 的计算结果为：<br>$$<br>G_y &#x3D;<br>\left[<br>\begin{matrix}<br>-1 &amp;-2 &amp;-1 \<br>0 &amp;0 &amp;0 \<br>+1 &amp;+2 &amp;+1<br>\end{matrix}<br>\right]</p>
<ul>
<li>I<br>$$</li>
</ul>
</li>
</ul>
<p>在图像的每一像素上，结合两个偏导结果求出近似梯度：<br>$$<br>G &#x3D; \sqrt{G_x^2 + G_y^2}<br>$$<br>有时也写为：<br>$$<br>G &#x3D; |G_x| + |G_y|<br>$$</p>
<h3 id="3-3-参考"><a href="#3-3-参考" class="headerlink" title="3.3  参考"></a>3.3  参考</h3><ul>
<li><a href="https://www.cnblogs.com/yibeimingyue/p/10878514.html">https://www.cnblogs.com/yibeimingyue/p/10878514.html</a></li>
<li><a href="https://blog.csdn.net/sundanping_123/article/details/86499500">https://blog.csdn.net/sundanping_123/article/details/86499500</a></li>
</ul>
<h2 id="4-TSDF（Truncated-Signed-Distance-Function）"><a href="#4-TSDF（Truncated-Signed-Distance-Function）" class="headerlink" title="4  TSDF（Truncated Signed Distance Function）"></a>4  TSDF（Truncated Signed Distance Function）</h2><h3 id="4-1-简介"><a href="#4-1-简介" class="headerlink" title="4.1  简介"></a>4.1  简介</h3><p>TSDF（基于截断的带符号距离函数）算法，是一种在 3D 重建中计算 iso-surface 的方法。在拥有大内存的显卡并行计算的情况下，可以做到实时重建的效果。</p>
<h3 id="4-2-算法思路"><a href="#4-2-算法思路" class="headerlink" title="4.2  算法思路"></a>4.2  算法思路</h3><p>用一个大空间（volume），这个空间可以完全包括要建立的 3D 模型，volume 由许多 voxel 构成：</p>
<center>
    <img src="./Mobile3DRecon/附-3 volume.png">
    <div>
        图-附 3  Volume
    </div>
</center>

<p>对每个 voxel（记为 $V$）对应空间中的一个点，这个点用两个量来评价：</p>
<ol>
<li>该 voxel 到最近的 surface（一开始不知道，设为没有）的距离，记为 $T(V)$，即带符号距离；</li>
<li>体素更新时的权重，记为 $W(V)$。</li>
</ol>
<center>
    <img src="./Mobile3DRecon/附-4 符号距离.png">
    <div>
        图-附 4  带符号距离
    </div>
</center>

<p>假设真实的平面到相机的深度为 $d_s$，相机采集到的深度为 $d_v$，那么符号距离就是<br>$$<br>d(V) &#x3D; d_s - d_v<br>$$<br>当 $d(x) &gt; 0$，说明体素在真实面的前面，否则在真实面后面。每次相机采集的数据都认为是最大可能的真实面，也有可能在数据的前后，但是概率要小。这个前后距离可以进行一定限制，如果距离特别远，那么概率会很小，就可以忽略。</p>
<h3 id="4-3-算法步骤"><a href="#4-3-算法步骤" class="headerlink" title="4.3  算法步骤"></a>4.3  算法步骤</h3><ol>
<li><p>准备：</p>
<ul>
<li>建立 volume，能够包围要重建的物体；</li>
<li>划分 voxel，其大小取决于 voxel 数目和 volume 的划分；</li>
<li>对于每个 voxel，转化其为世界坐标系下的 3D 位置点 $p$。</li>
</ul>
</li>
<li><p>遍历所有 voxel，计算 TSDF 值和权重：</p>
<ul>
<li><p>由深度数据的相机位姿矩阵，求世界坐标系下点 $p$ 在相机坐标系下的映射点 $v$，并由内参矩阵反投影 $v$ 求深度图像中对应的像素点 $x$，$x$ 的深度值记为 $value(x)$，点 $v$ 到相机坐标原点点距离为 $distance(v)$；</p>
</li>
<li><p>$p$ 的 SDF 值为 $sdf(p) &#x3D; value(x) - distance(v)$，设截断距离为 $u$，则有<br>  $$<br>  tsdf(p) &#x3D; \max (-1, \min(1, {sdf(P) \over u}))<br>  $$</p>
</li>
<li><p>计算权重：<br>  $$<br>  w(p) &#x3D; {\cos \theta \over distance(v)}<br>  $$<br>  其中 $\theta$ 为投影光线与 surface 法向量的夹角。</p>
</li>
</ul>
</li>
<li><p>当前帧与全局融合结果进行融合。</p>
<p> 如果当前帧是第一帧，则第一帧为融合结果；否则需要当前帧与之前的融合结果进行再融合。以 $TSDF(p)$ 为融合 TSDF 值，$W(p)$ 为融合权重，$tsdf(p)$ 为当前帧的 TSDF 值，$w(p)$ 为当前帧的权重值，则更新公式如下：<br> $$<br> TSDF_t (p) &#x3D; {W_{t-1} (p) TSDF_{t-1} (p) + w_t (p) tsdf_t (p) \over W_{t-1} (p) + w_t (p)} \<br> W_t (p) &#x3D; W_{t-1} (p) + w_t (p)<br> $$</p>
</li>
</ol>
<p>每添加一帧深度数据，就执行一遍 2，3 步，直到最后输出结果，给 Marching Cube 计算三角面。</p>
<h3 id="4-4-参考"><a href="#4-4-参考" class="headerlink" title="4.4  参考"></a>4.4  参考</h3><ul>
<li><a href="https://blog.csdn.net/zfjBIT/article/details/104648505">https://blog.csdn.net/zfjBIT/article/details/104648505</a></li>
<li><a href="https://jijianshu.com/p/fc53c02e9c76">https://jijianshu.com/p/fc53c02e9c76</a></li>
<li><a href="https://www.guyuehome.com/15664">https://www.guyuehome.com/15664</a></li>
<li><a href="https://blog.csdn.net/u011426016/article/details/103239696%EF%BC%88%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%EF%BC%89">https://blog.csdn.net/u011426016/article/details/103239696（源码解析）</a></li>
</ul>
<h2 id="5-Marching-Cubes-Algorithm"><a href="#5-Marching-Cubes-Algorithm" class="headerlink" title="5  Marching Cubes Algorithm"></a>5  Marching Cubes Algorithm</h2><h3 id="5-1-等值面（iso-surface）"><a href="#5-1-等值面（iso-surface）" class="headerlink" title="5.1  等值面（iso-surface）"></a>5.1  等值面（iso-surface）</h3><p>等值面是空间中的一个曲面，规定空间中每一个点都有一个属性值，属性值相等的连续空间组成的曲面，称之为等值面。</p>
<p>在采集深度点云时，由于传感器自身精度以及配准的误差在一个地方会采集到很多点，可以认为是对空间连续点的采样，而且点的位置会有很大的波动。通过一些其他方法可以得出这些点与真正的面的差值。那么真正的面在哪？</p>
<p>假如一个点在面的前方，标记为 $+1$，一个点在真实面的后方，标记为 $-1$，可以认为这个真正的面就在这两个点的中间。</p>
<p>还可以这么认为，空间内真正的面会产生一个场，场内有无数个等值面，这个值暂且称之为空间场值，空间场值在面的前方为正值，后方为负值。离真实面越近，空间场值的绝对值越小，真实面的空间场值为 $0$。目的就是在对这个场进行采样后，找到空间内真正的面。</p>
<h3 id="5-2-Marching-Cube（MC）算法简介"><a href="#5-2-Marching-Cube（MC）算法简介" class="headerlink" title="5.2  Marching Cube（MC）算法简介"></a>5.2  Marching Cube（MC）算法简介</h3><p>MC 是 W.Lorensen 等人于 1987 年提出来的一种体素级重建方法。MC 算法也被称为“等值面提取”（Iso-surface Extraction）算法。</p>
<h3 id="5-3-算法思路"><a href="#5-3-算法思路" class="headerlink" title="5.3  算法思路"></a>5.3  算法思路</h3><p>Marching Cube 首先将空间分成众多的六面体 cube，类似将空间分成很多的小块。然后找到等值面经过的 cube，如下图：</p>
<center>
    <img src="./Mobile3DRecon/附-5 求等值面.png">
    <div>
        图-附 5  cube 示意
    </div>
</center>

<p>求出该 cube 的边与 0 等值面（0 等值面就是空间场值为 0 的等值面）的交点，将这些交点按照一定的拓扑连接关系连接起来，作为 0 等值面在该 cube 中的近似表示。注意是近似表示，采用三角形。</p>
<h4 id="5-3-1-如何找到等值面经过的六面体网格"><a href="#5-3-1-如何找到等值面经过的六面体网格" class="headerlink" title="5.3.1  如何找到等值面经过的六面体网格"></a>5.3.1  如何找到等值面经过的六面体网格</h4><p>前面提到，有很多的已知采样点，并且知道这些点在空间中的空间场值，现在我们将空间分为很多个小格子，每个小格子都有 8 个顶点，通过计算 8 个顶点周围（范围与 cube 的大小相关）的采样点，近似计算出 8 个顶点的空间场值（加权平均等方法）。可以对 cube 做以下分类：</p>
<ul>
<li>8 个顶点都没有空间场值，0 等值面不经过或者经过但是没有采样，没有办法进行近似，所以不处理。</li>
<li>8 个顶点都有或部分有空间场值，当空间场值都是正值或者都是负值时，说明 0 等值面不在 cube 内。</li>
<li>当 cube 的顶点的空间场值有正有负时，则说明有 0 等直面穿过 cube。则需要从众多 cube 内，筛选出这样的 cube。</li>
</ul>
<h4 id="5-3-2-怎么求-cube-边与-0-等值面的交点"><a href="#5-3-2-怎么求-cube-边与-0-等值面的交点" class="headerlink" title="5.3.2  怎么求 cube 边与 0 等值面的交点"></a>5.3.2  怎么求 cube 边与 0 等值面的交点</h4><p>因为 cube 有 8 个顶点，8 个顶点的状态就有 $2^8  &#x3D; 256$ 种，但是由于 cube 具有对称性，通过对称性，可以简化这些状态至 15 种状态：</p>
<center>
    <img src="./Mobile3DRecon/附-6 cube的15种状态.png">
    <div>
        图-附 6  cube 的 15 种状态
    </div>
</center>

<p>现在需要精算出来这个交点的具体的位置，举一个例子，先给 cube 的所有的顶点和边标记：</p>
<center>
    <img src="./Mobile3DRecon/附-7 Marching Cube举例-1.png">
    <div>
        图-附 7  Marching Cube举例-1
    </div>
</center>

<p>假如体素下方的顶点 3 的值小于等值面值，其他顶点上的值都大于等值面值，那么可以生成一个与体素边 2，3，11 相交的三角面片，而三角面片顶点的具体位置则需要根据等值面值和边顶点 3-2，3-0，3-7 的值线性插值计算得到。</p>
<center>
    <img src="./Mobile3DRecon/附-8 Marching Cube举例-2.png">
    <div>
        图-附 8  Marching Cube举例-2
    </div>
</center>

<p>将交点坐标用 $P$ 表示，$P_1$、$P_2$ 代表边上两个端点的坐标，$V_1$、$V_2$ 代表这两个端点上的值，$V$ 代表等值面值，那么交点坐标的计算公式如下（线性插值，也可以是其他方法）：<br>$$<br>P &#x3D; P_1 + {V - V_1 \over V_2 - V_1} (P_2 - P_1)<br>$$<br>那么现在对所有的含有等值面的 cube 计算出了其与等值面的交点。最后一步，就是将这些交点连接成三角形。</p>
<h4 id="5-3-3-这些交点按照怎么的拓扑连接关系连接，是怎么操作的？"><a href="#5-3-3-这些交点按照怎么的拓扑连接关系连接，是怎么操作的？" class="headerlink" title="5.3.3  这些交点按照怎么的拓扑连接关系连接，是怎么操作的？"></a>5.3.3  这些交点按照怎么的拓扑连接关系连接，是怎么操作的？</h4><p>连接三角形的拓扑结构，也在前面 15 中情况中，都画了出来，只要按照上图的表中已经记录好的连接关系，就可以将三角形连接出来。</p>
<h3 id="5-4-算法流程"><a href="#5-4-算法流程" class="headerlink" title="5.4  算法流程"></a>5.4  算法流程</h3><ol>
<li><p>将原始数据经过预处理之后，读入特定的数组或者八叉树中。</p>
</li>
<li><p>从 volume 中提取一个 cube，成为当前 cube 同时获取该 cube 的所有信息，例如 8 个顶点的值，坐标位置等。</p>
</li>
<li><p>将当前 cube 的 8 个顶点的函数值与给定等值面值 C 进行比较,得到该 cube 的状态表。</p>
</li>
<li><p>根据当前 cube 的状态表索引，找出与等值面相交的 cube 棱边，并采用线性插值的方法计算出各个交点的位置坐标。</p>
</li>
<li><p>利用中心差分法，求出当前 cube 的 8 个顶点的法向量，采用线性插值的方法，得到三角面各个顶点的法向量。</p>
</li>
<li><p>根据各个三角面顶点的坐标、顶点法向量进行三角面的连接。</p>
</li>
</ol>
<h3 id="5-5-参考"><a href="#5-5-参考" class="headerlink" title="5.5  参考"></a>5.5  参考</h3><ul>
<li><a href="https://www.jianshu.com/p/5a6ade7b77b6">https://www.jianshu.com/p/5a6ade7b77b6</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/48022195">https://zhuanlan.zhihu.com/p/48022195</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/45292978%EF%BC%88%E8%BF%9E%E6%8E%A5">https://zhuanlan.zhihu.com/p/45292978（连接</a> TSDF 和 Marching Cube）</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li>[1] <a href="http://openar.sensetime.com/">http://openar.sensetime.com</a></li>
<li>[2] H. Hirschmuller. Accurate and efficient stereo processing by semi-global matching and mutual information. In <em>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</em>, vol. 2, pp. 807–814, 2005.</li>
<li>[3] H. Hirschmuller. Stereo processing by semiglobal matching and mutual information. <em>IEEE Transactions on Pattern Analysis and Machine Intelli- gence</em>, 30(2):328–341, 2007.</li>
<li>[4] A. Drory, C. Haubold, S. Avidan, and F. A. Hamprecht. Semi-global matching: A principled derivation in terms of message passing. In <em>German Conference on Pattern Recognition</em>, pp. 43–53. Springer, 2014.</li>
<li>[5] C. Godard, O. Mac Aodha, and G. J. Brostow. Unsupervised monocular depth estimation with left-right consistency. In <em>IEEE Conference on Computer Vision and Pattern Recognition</em>, pp. 270–279, 2017.</li>
<li>[6] J. Hu, M. Ozay, Y. Zhang, and T. Okatani. Revisiting single image depth estimation: Toward higher resolution maps with accurate object bound-aries. In <em>IEEE Winter Conference on Applications of Computer Vision</em>, pp. 1043–1051, 2019.</li>
<li>[7] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In <em>European Conferenceon Computer Vision</em>, pp. 694–711. Springer, 2016.</li>
</ul>
]]></content>
      <categories>
        <category>Paper Note</category>
      </categories>
      <tags>
        <tag>Paper Note</tag>
        <tag>K-core</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello-world</title>
    <url>/papernote/Hello-world/</url>
    <content><![CDATA[<p>对论文 hello world的整理</p>
<span id="more"></span>

<center><b>

<h1 id="论文情况"><a href="#论文情况" class="headerlink" title="论文情况"></a>论文情况</h1><ul>
<li>标题：Mobile3DRecon:Real-time Monocular 3D Reconstruction on a Mobile Phone</li>
<li>作者：Xingbin Yang, Liyang Zhou, Hanqing Jiang, and etc.</li>
<li>会议：IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS (TVCG) 2020</li>
<li>源码：（未开源，官方网站）<a href="https://zju3dv.github.io/mobile3drecon.github.io/">https://zju3dv.github.io/mobile3drecon.github.io/</a></li>
</ul>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1  Introduction"></a>1  Introduction</h1><p><mjx-container class="MathJax" jax="SVG" display="true" width="full" style="min-width: 41.989ex;"><svg style="vertical-align: -2.007ex; min-width: 41.989ex;" xmlns="http://www.w3.org/2000/svg" width="100%" height="5.144ex" role="img" focusable="false"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(0.0181,-0.0181) translate(0, -1386.9)"><g data-mml-node="math"><g data-mml-node="mtable" transform="translate(2078,0) translate(-2078,0)"><g transform="translate(0 1386.9) matrix(1 0 0 -1 0 0) scale(55.25)"><svg data-table="true" preserveAspectRatio="xMidYMid" viewBox="7201.5 -1386.9 1 2273.9"><g transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="mlabeledtr" transform="translate(0,-157)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(749,-150) scale(0.707)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g><g data-mml-node="mo" transform="translate(1102.3,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1491.3,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(1852.3,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msup" transform="translate(2297,0)"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(394,413) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g><g data-mml-node="mo" transform="translate(2935.5,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3602.2,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(4658,0)"><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z"></path><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(444,0)"></path><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(972,0)"></path></g><g data-mml-node="mo" transform="translate(6186,0)"><path data-c="2061" d=""></path></g><g data-mml-node="mo" transform="translate(6186,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mo" transform="translate(6575,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(7353,0)"><g data-mml-node="mfrac"><g data-mml-node="mrow" transform="translate(220,710)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mo" transform="translate(818,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1207,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(1568,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msup" transform="translate(2012.7,0)"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(394,363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g><g data-mml-node="mo" transform="translate(2651.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3262.3,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(4262.6,0)"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mi" transform="translate(462,-150) scale(0.707)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="msup" transform="translate(5395.4,0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mn" transform="translate(422,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g><g data-mml-node="msup" transform="translate(2890.2,-719.9)"><g data-mml-node="mi"><path data-c="1D6FF" d="M195 609Q195 656 227 686T302 717Q319 716 351 709T407 697T433 690Q451 682 451 662Q451 644 438 628T403 612Q382 612 348 641T288 671T249 657T235 628Q235 584 334 463Q401 379 401 292Q401 169 340 80T205 -10H198Q127 -10 83 36T36 153Q36 286 151 382Q191 413 252 434Q252 435 245 449T230 481T214 521T201 566T195 609ZM112 130Q112 83 136 55T204 27Q233 27 256 51T291 111T309 178T316 232Q316 267 309 298T295 344T269 400L259 396Q215 381 183 342T137 256T118 179T112 130Z"></path></g><g data-mml-node="mn" transform="translate(477,289) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><rect width="6421" height="60" x="120" y="220"></rect></g></g><g data-mml-node="mo" transform="translate(14014,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></g></svg><svg data-labels="true" preserveAspectRatio="xMaxYMid" viewBox="1278 -1386.9 1 2273.9"><g data-labels="true" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="mtd" id="mjx-eqn:1" transform="translate(0,593)"><text data-id-align="true"></text><g data-idbox="true" transform="translate(0,-750)"><g data-mml-node="mtext"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" transform="translate(389,0)"></path><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" transform="translate(889,0)"></path></g></g></g></g></svg></g></g></g></g></svg></mjx-container></p>
</b></center>]]></content>
      <categories>
        <category>Paper Note</category>
      </categories>
      <tags>
        <tag>Paper Note</tag>
        <tag>Hello World</tag>
      </tags>
  </entry>
</search>
